import openai
from openai import OpenAI
import os
import json
import Prompts
import ApiKey

# Set the API key and environment variable
# TODO : In an actual production environment, we will have to set the API key in the environment variable, or it will have to be decided later
openai.api_key = ApiKey.API_KEY
os.environ["OPENAI_API_KEY"] = ApiKey.API_KEY
client = OpenAI()

# This will be function that will get the GPT Function
def getGPTFunctions():
    return [
        {
            "type" : "function",
            "function" : {
                "name" : "get_batch_response",
                "description" : Prompts.FUNCTION_BATCH_DESCRIPTION,
                "parameters":{
                    "type" : "object",
                    "properties" : {
                        "batch_file" : {
                            "description" : Prompts.BATCH_DICTIONARY_DESCRIPTION
                        },
                        "message" : {
                            "description" : Prompts.MESSAGE_DICTIONARY_DESCRIPTION,
                        }
                    }
                }
            },
            "required" : ["batch_file","message"]
        },
        {
            "type" : "function",
            "function" : {
                "name" : "get_general_response",
                "description" : Prompts.FUNCTION_GENERAL_DESCTIPRION,
                "parameters":{
                    "type" : "object",
                    "properties" : {
                        "general_response" : {
                            "description" : Prompts.RESPONSE_DICTIONARY_DESCRIPTION
                        }
                    }
                }
            },  
            "required" : ["general_response"]      
        }
    ]

# Function to call GPT-3/GPT-4  
def getGPTResponse(systemPrompt, userInput):
    try:
        completion = client.chat.completions.create(
        model = "gpt-3.5-turbo",
        response_format = {"type" : "json_object"},
        messages = [
            {"role": "assistant", "content": systemPrompt},
            {"role": "user", "content": userInput}
        ],
        tools = getGPTFunctions()
        )

        # This is the actual response
        actualResponse = completion.choices[0].message.tool_calls[0].function.arguments

        # Lets convert it into the json format
        return json.loads(actualResponse)
        
    except Exception as e:
        # These error types are still in prototype, we will have to check with the frontend to determine what type of errors they want
        raise RuntimeError("Error in GPT call : " + str(e))
    
# This will be the function to format the response generated by the GPT Model
def formatResponse(response):
    # First we will check if the response is of the general prompt type
    if "general_response" in response:
        return {"response" : response["general_response"], "type" : "general", "status" : 200}
    
    # If the response is of the batch prompt type, we will return the batch file and the message
    if "batch_file" in response and "message" in response:
        return {"batch_file" : response["batch_file"], "message" : response["message"], "type" : "batch", "status" : 200}

    # If none of this happens we will call the error function
    else:
        return {"error" : "Error in getting the response", "status" : 400}
    

# This will be the function that will be called to get the response from the GPT Model
def getResponse():
    # This will be the function that will be called to store the response from the GPT Model
    response = getGPTResponse(Prompts.SYSTEM_PROMPT, Prompts.USER_PROMPT)

    # We will now return the formatted response
    return formatResponse(response)

if __name__ == "__main__":
    # Lets test the function
    getResponse()